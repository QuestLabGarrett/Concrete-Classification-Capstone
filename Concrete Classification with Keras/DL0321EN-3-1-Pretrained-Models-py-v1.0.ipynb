{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dir = '../resources/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical',\n",
    "    subset='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 21s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.engine.functional.Functional at 0x2b0b6a61d50>,\n",
       " <keras.src.layers.core.dense.Dense at 0x2b0b6a6bd90>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.engine.input_layer.InputLayer at 0x2b0b64bfc50>,\n",
       " <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x2b0b51a8ed0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b51bac10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6c27990>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6cb0690>,\n",
       " <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x2b0b6c8d050>,\n",
       " <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x2b0b6c29050>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6c84550>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b3388d50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6c5b590>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6c3d6d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6c8e510>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6c18310>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b8440250>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b651e950>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6c614d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6d33bd0>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b684f2d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6c19d10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6d592d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6d51250>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6d3e550>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6d60fd0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6c54110>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6d86ad0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6db0a10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6c9b650>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b6d96ed0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6d87cd0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6dec210>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0f26acf90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6dc1090>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6ded210>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6512cd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b50f9010>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b4d79b50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6db9090>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b6dee010>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b65c13d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b4e7e510>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6e1fc10>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6c4eb90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6e2ea90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6e34ed0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6e424d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6df7c90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b685ce10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6dac650>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6cc8f10>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b6db1890>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6e6a150>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6eafd50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6e97010>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6eb7d10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6d5a650>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6ebd590>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6e7fa10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6ea9690>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6e2eb90>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b6ea0790>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b5cfa210>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6da4a50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6c6f3d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6d51b50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6c7db10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6df6790>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6eb6850>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6f109d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6f183d0>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b8442fd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6ef9d50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b684fd90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6f2bc50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6f12d10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b7074790>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b705bed0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b7073d90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b70b9250>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b70b6a50>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b70d2f10>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b70db110>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6f1bc90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b7094f50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6dafed0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6df4e10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6e57850>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6e0bc50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b70a1610>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6ea88d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b70965d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b710f990>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b7045810>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6ddcbd0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b70a1f10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b4d57c50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6571190>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6554210>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6545650>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b714e0d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b716a650>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6494d50>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b70d9e50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b654fd90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b51bab50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6887e10>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6886a10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b713d0d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b7124650>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6884450>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b712bd50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6e665d0>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b6cfedd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6f28d50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b7186710>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b710fc90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b7125590>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b68ca690>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b716b010>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b719af50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b68dc910>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b68d4690>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b7182950>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b719de90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b71c45d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b68ee1d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b71c6b90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b71ea490>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b71770d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b691c7d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b72212d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b7221a90>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b68fd750>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b68f6950>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b7199490>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b84437d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b7222090>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6d23850>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b70a1ad0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b688af50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6931690>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b7089750>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b67b8d50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b693b090>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6952850>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6965390>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b69523d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6f3fed0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6981250>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6f3fbd0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6952490>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6983a10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6950590>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b71ea450>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b6f5ab90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6f58890>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6f475d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b699e590>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b69a4510>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b694ac10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b693b290>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b699ea10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b70e6ad0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b7198a10>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b695d510>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b71930d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b69d5010>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6f3c510>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b69cc790>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b69ddcd0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b69d7ed0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6970b50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2b0b6a07f90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2b0b6a20710>,\n",
       " <keras.src.layers.merging.add.Add at 0x2b0b7128c50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2b0b6a04750>,\n",
       " <keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x2b0b6a53890>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23591810 (90.00 MB)\n",
      "Trainable params: 4098 (16.01 KB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garrett\\AppData\\Local\\Temp\\ipykernel_9220\\251737888.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  fit_history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "300/300 [==============================] - 1402s 5s/step - loss: 0.0368 - accuracy: 0.9886 - val_loss: 0.0076 - val_accuracy: 0.9982\n",
      "Epoch 2/2\n",
      "300/300 [==============================] - 1425s 5s/step - loss: 0.0070 - accuracy: 0.9984 - val_loss: 0.0049 - val_accuracy: 0.9987\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garrett\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
